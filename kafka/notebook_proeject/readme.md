
 ![image](https://user-images.githubusercontent.com/76681523/207795887-6ed61a30-ba16-4a6a-a576-713d5cdbc0c6.png)


- 중고나라의 중고 노트북 판매/구매 게시판에 올라오는 게시물을 실시간으로 각 브랜드에 맞는 테이블로 정보를 적재하는 형식입니다.
- 운영 중인 서비스에 접근하지 않는 이상 kafka를 활용한 streaming process를 구현하기 어렵다는 문제가 있었습니다. 이에 우리나라에서 가장 활발하게 거래가 이루어지는 중고나라 게시물을 스크랩하기로 생각하게 되었습니다.
- 실제로 짧게는 1분, 길게는 3분 간격으로 게시물이 끊임없이 갱신되고 있어 이는 효과적인 전략이었습니다.


## 설계 설명

1. 웹에서 데이터를 받아온 뒤 중계 역할을 하는 topic으로 데이터를 보내고
2. 이후 중계 단계에서 각 브랜드에 맞는 topic으로 데이터를 전송, 
3. mysql database에 각 브랜드 별 판매 데이터가 구분되어 적재되는 형태

## 최종목표

- 본래의 최종 목표는 각 브랜드의 세부제품들이 중고제품 시장에서 팔리는 가격의 추세를 살피고 이를 시각화하는 것이었지만, 예상 외로 세부제품이 너무나도 다양하고, 이를 무시하는 것 또한 무의미한 일이라고 생각해 일단 어떻게 처리한 뒤 저장할 것인가에 집중한 프로젝트입니다.

## 트러블 슈팅
1. 접속의 불안정성
- 웹에서 데이터를 끌어오는 만큼, 웹으로부터 종종 차단되는 경우가 발생.
 - 처음에는 접속이 차단되지 않게끔 만들고자 노력했으나, 언젠가는 무조건 걸린다는 것만 알게 되었음. 
 - 따라서 이후로는 접속이 때때로 차단된다는 것을 당연한 것으로 여기고, 차단되었을 때 작업이 중지되지 않고 계속해서 지속될 수 있게끔 만들고자 하였음.
 - 결과적으로 차단 여부와 관계없이 게시물 갱신 때 데이터를 가지고 올 수 있게 되었음.

2. topic 별 데이터 분배
- 데이터를 분배하고자 했을 때 처음에는 중간다리를 만들지 않고 한번에 db로 적재하고자 하였음.
- 하지만 이 방법은 코드가 지나치게 난잡해지는 문제점을 가지고 있었음.
- 때문에 중간다리를 만든 뒤에 모든 데이터를 'test'라는 토픽으로 보내 중간 과정을 거치게 하였고, 그 과정 속에서 각각 브랜드에 맞는 토픽을 만들어 다시 db 쪽으로 producing 하는 방법으로 바꾸게 되었음.
- 결론적으로 코드가 훨씬 가독성 있게 변화하였음. 


 ## 실행 방법
 
 #### 파일 설명
 1. f-4.py : 중고나라 게시판에서 게시물 정보를 가지고 오는 역할
 2. intermediate_stage.py : 각 브랜드에 맞는 토픽으로 정보들을 분배하는 역할
 3. brands_db 하 파일들(chanel.py,louis.py 등등..) : postgresql의 각 테이블에 정보들을 적재하는 역할
 
 - 위 파일들을 통해, 카프카의 zookeeper과 broker을 실행시킨 후 python을 이용해 실행시킬 수 있게 된다. 

#### 처리 결과

- 처리 과정
- ![image](https://user-images.githubusercontent.com/76681523/207798475-e27facef-01ee-4388-a918-77f47d8a7e3a.png)


- 처리결과(db적재)
- ![image](https://user-images.githubusercontent.com/76681523/207798172-78b132b1-3d06-46b1-ab49-5b28b66579f7.png)


