
 ![image](https://user-images.githubusercontent.com/76681523/207795887-6ed61a30-ba16-4a6a-a576-713d5cdbc0c6.png)


- 중고나라 명품가방 게시판에 올라오는 게시물을 실시간으로 각 브랜드에 맞는 테이블로 정보를 적재하는 형식

  - 운영 중인 서비스에 접근하지 않는 이상 kafka를 활용한 streaming process를 구현하기 어려워, 우리나라에서 가장 활발하게 거래가 이루어지는 중고나라 게시물을 스크랩하기로 생각하게 되었음. 실제로 명품가방 게시판에서는 짧게는 1분, 길게는 3분 간격으로 게시물이 끊임없이 갱신되고 있어 이는 효과적인 전략이었음. 


## 설계 설명

- beautifulsoup를 통해 웹에서 데이터를 받아온 뒤 중계 역할을 하는 topic으로 데이터를 보내고
- 이후 중계 단계에서 각 브랜드에 맞는 topic으로 데이터를 전송, 
- postgresql db에 데이터가 적재되는 형태


- 본래의 최종 목표는 각 브랜드의 세부제품들이 중고제품 시장에서 팔리는 가격의 추세를 살피고 이를 시각화하는 것이었지만, 예상 외로 세부제품이 너무나도 다양하고, 이를 무시하는 것 또한 무의미한 일이라고 생각해 일단 db에 적재하는 수준까지 구현한 상태.
  - 세부적인 접근을 어떻게 해야할지만 만들어진다면, 적재된 데이터를 처리해 시각화하는 것은 kibanna 혹은 superset을 통해 간단히 가능할 것이라고 생각.
 
 
 ## 실행 방법
 
 #### 파일 설명
 1. producer.py : 중고나라 게시판에서 게시물 정보를 가지고 오는 역할
 2. intermediate_stage.py : 각 브랜드에 맞는 토픽으로 정보들을 분배하는 역할
 3. brands_db 하 파일들(chanel.py,louis.py 등등..) : postgresql의 각 테이블에 정보들을 적재하는 역할
 
 - 위 파일들을 통해, 카프카의 zookeeper과 broker을 실행시킨 후 python을 이용해 실행시킬 수 있게 된다. 

#### 처리 결과
![image](https://user-images.githubusercontent.com/76681523/193805623-43ea1584-3d32-4392-8c34-756b0210fe4b.png)


- 앞으로의 목표
  - 구체적인 분석목표 설정
    - 정 생각나는게 없으면 다른 아이템을 선정해 다시 적재해야 할 것이다. 여기까지 구현했으면 거의 다 한건데,, 조금 아깝다.
    - 머신러닝에 관한 방향성이 있을까? 
    - 시각화 할 수 있는 것은?
    - 적재에 성공한 이상, 분석에 필요한 정보를 더 수집하는 것은 쉬운 일일것이다. 가격, 세부적인 제품군 등등... 하지만 문제는 목표가 없다. 
