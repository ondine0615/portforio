
 ![image](https://user-images.githubusercontent.com/76681523/207795887-6ed61a30-ba16-4a6a-a576-713d5cdbc0c6.png)


- 중고나라의 중고 노트북 판매/구매 게시판에 올라오는 게시물을 실시간으로 각 브랜드에 맞는 테이블로 정보를 적재하는 형식입니다.
- 운영 중인 서비스에 접근하지 않는 이상 kafka를 활용한 streaming process를 구현하기 어렵다는 문제가 있었습니다. 이에 우리나라에서 가장 활발하게 거래가 이루어지는 중고나라 게시물을 스크랩하기로 생각하게 되었습니다.
- 실제로 짧게는 1분, 길게는 3분 간격으로 게시물이 끊임없이 갱신되고 있어 이는 효과적인 전략이었습니다.


## 설계 설명

1. 웹에서 데이터를 받아온 뒤 중계 역할을 하는 topic으로 데이터를 보내고
2. 이후 중계 단계에서 각 브랜드에 맞는 topic으로 데이터를 전송, 
3. mysql database에 각 브랜드 별 판매 데이터가 구분되어 적재되는 형태를 띈다.

## 최종목표

- 본래의 최종 목표는 각 브랜드의 세부제품들이 중고제품 시장에서 팔리는 가격의 추세를 살피고 이를 시각화하는 것이었지만, 예상 외로 세부제품이 너무나도 다양하고, 이를 무시하는 것 또한 무의미한 일이라고 생각해 일단 db에 적재하는 수준까지 구현한 상태라고 할 수 있다. 
- 세부적인 접근을 어떻게 해야할지만 만들어진다면, 적재된 데이터를 처리해 시각화하는 것은 kibanna 혹은 superset을 통해 간단히 가능할 것이라고 생각.
 
 
 ## 실행 방법
 
 #### 파일 설명
 1. producer.py : 중고나라 게시판에서 게시물 정보를 가지고 오는 역할
 2. intermediate_stage.py : 각 브랜드에 맞는 토픽으로 정보들을 분배하는 역할
 3. brands_db 하 파일들(chanel.py,louis.py 등등..) : postgresql의 각 테이블에 정보들을 적재하는 역할
 
 - 위 파일들을 통해, 카프카의 zookeeper과 broker을 실행시킨 후 python을 이용해 실행시킬 수 있게 된다. 

#### 처리 결과

- 처리 과정
- ![image](https://user-images.githubusercontent.com/76681523/207798475-e27facef-01ee-4388-a918-77f47d8a7e3a.png)


- 처리결과(db적재)
- ![image](https://user-images.githubusercontent.com/76681523/207798172-78b132b1-3d06-46b1-ab49-5b28b66579f7.png)


